{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a999b204-3b95-404f-93a1-90b1bda33abb",
   "metadata": {},
   "source": [
    "<img src=\"../assets/notebook.png\" />\n",
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Generative Models For Spatial-Based Bottom <br><br> Hypoxia Forecasting In The Black Sea</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">\n",
    "\n",
    "Do not forget to activate the **environnement esa_diffusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55dcb8-9067-463c-b876-2e45565d6240",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ---------- Librairies ----------\n",
    "import os\n",
    "import sys\n",
    "import dawgz\n",
    "import wandb\n",
    "import xarray\n",
    "import random\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specific\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dawgz (jobs //)\n",
    "from dawgz import job, schedule\n",
    "\n",
    "# ---------- Librairies (Custom) ----------\n",
    "#\n",
    "# Adding path to source folder to load custom modules\n",
    "sys.path.append('/src')\n",
    "sys.path.append('/src/debs/')\n",
    "sys.path.insert(1, '/src/debs/')\n",
    "sys.path.insert(1, '/scripts/')\n",
    "\n",
    "# Moving to the .py directory\n",
    "%cd src/debs/\n",
    "\n",
    "## Loading libraries\n",
    "from dataset     import *\n",
    "from dataloader  import *\n",
    "from training    import *\n",
    "from metrics     import *\n",
    "from tools       import *\n",
    "from unet        import *\n",
    "\n",
    "# ---------- Jupyter ----------\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "# Making sure modules are reloaded when modified\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c007b-1cab-4407-a5f0-cbcdd836a569",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Scripts</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">\n",
    "\n",
    "\n",
    "The configurations are available in the **configs** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Neural Network\n",
    "%run __training.py --config local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea82c99",
   "metadata": {},
   "source": [
    "<hr style=\"color:#5A7D9F;\">\n",
    "<p align=\"center\">\n",
    "    <b style=\"font-size:2vw; color:#5A7D9F; font-weight:bold;\">\n",
    "    <center>Playground</center>\n",
    "    </b>\n",
    "</p>\n",
    "<hr style=\"color:#5A7D9F;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fe13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Paremeters\n",
    "kwargs = {\n",
    "    \"Project\"                : \"ESA - Notebook (Diffusion)\",\n",
    "    \"Mode\"                   : \"disabled\",\n",
    "    \"Window (Inputs)\"        : 1,\n",
    "    \"Window (Outputs)\"       : 10,\n",
    "    \"Diffusion Steps\"        : 5,\n",
    "    \"Diffusion Scheduler\"    : 0.0125,\n",
    "    \"Diffusion Variance\"     : 0.005,\n",
    "    \"Frequencies\"            : 32,\n",
    "    \"Scaling\"                : 1,\n",
    "    \"Learning Rate\"          : 0.0001,\n",
    "    \"Batch Size\"             : 16,\n",
    "    \"Epochs\"                 : 1,\n",
    "    'Number of Workers'      : 2,\n",
    "    'Results (Epoch)'        : 1,\n",
    "    'Results (Trajectories)' : 10,\n",
    "    'Model Saving'           : False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c08286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------—---------\n",
    "#     Initialization\n",
    "# -------------—---------\n",
    "#\n",
    "# Information over terminal (1)\n",
    "project_title(kwargs)\n",
    "\n",
    "# Checking if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Fixing random seed for reproducibility\n",
    "np.random.seed(2701)\n",
    "torch.manual_seed(2701)\n",
    "\n",
    "# Loading configuration\n",
    "project        = kwargs['Project']\n",
    "mode           = kwargs['Mode']\n",
    "window_input   = kwargs['Window (Inputs)']\n",
    "window_output  = kwargs['Window (Outputs)']\n",
    "diff_steps     = kwargs['Diffusion Steps']\n",
    "diff_scheduler = kwargs['Diffusion Scheduler']\n",
    "diff_variance  = kwargs['Diffusion Variance']\n",
    "scaling        = kwargs['Scaling']\n",
    "frequencies    = kwargs['Frequencies']\n",
    "learning_rate  = kwargs['Learning Rate']\n",
    "batch_size     = kwargs['Batch Size']\n",
    "nb_epochs      = kwargs['Epochs']\n",
    "num_workers    = kwargs['Number of Workers']\n",
    "model_saving   = kwargs['Model Saving']\n",
    "results_epoch  = kwargs['Results (Epoch)']\n",
    "results_traj   = kwargs['Results (Trajectories)']\n",
    "\n",
    "# -------------—---------\n",
    "#          Data\n",
    "# -----------------------\n",
    "#\n",
    "# Loading preprocessed datasets\n",
    "dataset_train      = BlackSea_Dataset(\"Test\")\n",
    "# dataset_validation = BlackSea_Dataset(\"Test\")\n",
    "\n",
    "# Loading other information\n",
    "black_sea_mesh       = dataset_train.get_mesh()\n",
    "black_sea_mask       = dataset_train.get_mask(continental_shelf = False)\n",
    "black_sea_mask_cs    = dataset_train.get_mask(continental_shelf = True)\n",
    "black_sea_bathymetry = dataset_train.get_depth(unit = \"meter\")\n",
    "\n",
    "# Used to detect the presence of hypoxia events\n",
    "hypoxia_treshold_standardized = dataset_train.get_treshold(standardized = True)\n",
    "\n",
    "# Creation of the dataloaders\n",
    "dataloader_train = BlackSea_Dataloader(dataset_train,\n",
    "                                       window_input,\n",
    "                                       window_output,\n",
    "                                       frequencies,\n",
    "                                       batch_size,\n",
    "                                       num_workers,\n",
    "                                       black_sea_mesh,\n",
    "                                       black_sea_mask,\n",
    "                                       black_sea_mask_cs,\n",
    "                                       black_sea_bathymetry,\n",
    "                                       random = True).get_dataloader()\n",
    "\n",
    "dataloader_valid = BlackSea_Dataloader_Diffusion(dataset_train,\n",
    "                                                 window_input,\n",
    "                                                 window_output,\n",
    "                                                 frequencies,\n",
    "                                                 12,\n",
    "                                                 num_workers,\n",
    "                                                 black_sea_mesh,\n",
    "                                                 black_sea_mask,\n",
    "                                                 black_sea_mask_cs,\n",
    "                                                 black_sea_bathymetry,\n",
    "                                                 random = False).get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "\n",
    "def time_encoding(time: torch.Tensor, frequencies:int = 128):\n",
    "    r\"\"\"Encoding the time using the \"Attention is all you need\" paper encoding scheme\"\"\"\n",
    "\n",
    "    # Security\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Encoding functions\n",
    "        sinusoidal   = lambda time, frequency_index, frequencies: torch.sin(time / (10000 ** (frequency_index / frequencies)))\n",
    "        cosinusoidal = lambda time, frequency_index, frequencies: torch.cos(time / (10000 ** (frequency_index / frequencies)))\n",
    "\n",
    "        # Storing the encoding\n",
    "        encoded_time = torch.zeros(time.shape[0], time.shape[1], frequencies * 2)\n",
    "\n",
    "        # Mapping time to its encoding\n",
    "        for b_index, b in enumerate(time):\n",
    "            for t_index, t in enumerate(b):\n",
    "\n",
    "                # Stores the current encoding\n",
    "                encoding = list()\n",
    "\n",
    "                # Computing the encoding, i.e. alternating between sinusoidal and cosinusoidal encoding\n",
    "                for i in range(frequencies):\n",
    "                    encoding += [sinusoidal(t, i, frequencies), cosinusoidal(t, i, frequencies)]\n",
    "\n",
    "                # Conversion to torch tensor and storing the encoding\n",
    "                encoded_time[b_index, t_index, :] =  torch.FloatTensor(encoding).clone()\n",
    "\n",
    "        return encoded_time\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    r\"\"\"Custom Layer Normalization module\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        var, mean = torch.var_mean(x, dim = self.dim, keepdim = True)\n",
    "        return (x - mean)/torch.sqrt(var + self.eps)\n",
    "\n",
    "class TimeResidual_Block(nn.Module):\n",
    "    r\"\"\"A time residual block for UNET\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels: int, frequencies: int):\n",
    "        super(TimeResidual_Block, self).__init__()\n",
    "\n",
    "        # Initializations\n",
    "        self.frequencies   = frequencies\n",
    "        self.activation    = nn.SiLU()\n",
    "        self.normalization = LayerNormalization(dim = 1)\n",
    "        self.variance      = torch.sqrt(torch.tensor(2))\n",
    "\n",
    "        # Temporal Projection on the channels\n",
    "        self.time_projection = nn.Linear(in_features = self.frequencies * 2, out_features = input_channels, bias = False)\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv1 = nn.Conv2d(in_channels  = input_channels,\n",
    "                               out_channels = input_channels,\n",
    "                               kernel_size  = 3,\n",
    "                               stride       = 1,\n",
    "                               padding      = 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels  = input_channels,\n",
    "                               out_channels = input_channels,\n",
    "                               kernel_size  = 3,\n",
    "                               stride       = 1,\n",
    "                               padding      = 1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "\n",
    "        # -------------------\n",
    "        #        Time\n",
    "        # -------------------\n",
    "        # 1. Initial information\n",
    "        b, c, x_res, y_res = x.shape\n",
    "\n",
    "        # 3. Temporal Projection\n",
    "        encoded_time = self.time_projection(time)\n",
    "        encoded_time = self.activation(encoded_time)\n",
    "\n",
    "        # 4. Reshaping the time encoding\n",
    "        encoded_time = encoded_time[:, :, None, None]\n",
    "\n",
    "        # 5. Creating the grids\n",
    "        encoded_time = encoded_time.expand(-1, -1, x_res, y_res)\n",
    "\n",
    "        # -------------------\n",
    "        #        Spatial\n",
    "        # -------------------\n",
    "        # 1. Adding temporal information (broadcasting)\n",
    "        x_residual = x + encoded_time\n",
    "\n",
    "        # 2. Normalization\n",
    "        x_residual = self.normalization(x_residual)\n",
    "\n",
    "        # 3. Convolution (1)\n",
    "        x_residual = self.conv1(x_residual)\n",
    "\n",
    "        # 4. Activation\n",
    "        x_residual = self.activation(x_residual)\n",
    "\n",
    "        # 5. Convolution (2)\n",
    "        x_residual = self.conv2(x_residual)\n",
    "\n",
    "        # 6. Adding the residual\n",
    "        x = x + x_residual\n",
    "\n",
    "        # 7. Keeping unit variance\n",
    "        return x / self.variance\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "\n",
    "class TimeResidual_UNET(nn.Module):\n",
    "    r\"\"\"A time residual UNET for time series forecasting\"\"\"\n",
    "\n",
    "    def __init__(self, input_channels: int, output_channels: int, frequencies: int, scaling: int = 1):\n",
    "        super(TimeResidual_UNET, self).__init__()\n",
    "\n",
    "        # Initializations\n",
    "        self.frequencies      = frequencies\n",
    "        self.input_channels   = input_channels\n",
    "        self.output_channels  = output_channels\n",
    "\n",
    "        # 1. Input (lifting)\n",
    "        self.input_conv = nn.Conv2d(in_channels = self.input_channels, out_channels = 32 * scaling, kernel_size = 3, stride = 1, padding = 1)\n",
    "\n",
    "        # 2. Downsampling\n",
    "        #\n",
    "        # Time Residual Blocks (1)\n",
    "        self.downsample_11_residuals = TimeResidual_Block(input_channels  = 32 * scaling,     frequencies = self.frequencies)\n",
    "        self.downsample_12_residuals = TimeResidual_Block(input_channels  = 32 * scaling,     frequencies = self.frequencies)\n",
    "        self.downsample_21_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 2, frequencies = self.frequencies)\n",
    "        self.downsample_22_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 2, frequencies = self.frequencies)\n",
    "        self.downsample_31_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 4, frequencies = self.frequencies)\n",
    "        self.downsample_32_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 4, frequencies = self.frequencies)\n",
    "        self.downsample_41_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 8, frequencies = self.frequencies)\n",
    "        self.downsample_42_residuals = TimeResidual_Block(input_channels  = 32 * scaling * 8, frequencies = self.frequencies)\n",
    "\n",
    "        # Convolutions (downsampling)\n",
    "        self.downsample_1_conv = nn.Conv2d(in_channels = 32 * scaling,     out_channels = 32 * scaling * 2, kernel_size = 2, stride = 2)\n",
    "        self.downsample_2_conv = nn.Conv2d(in_channels = 32 * scaling * 2, out_channels = 32 * scaling * 4, kernel_size = 2, stride = 2)\n",
    "        self.downsample_3_conv = nn.Conv2d(in_channels = 32 * scaling * 4, out_channels = 32 * scaling * 8, kernel_size = 2, stride = 2)\n",
    "\n",
    "        # 3. Upsampling\n",
    "        #\n",
    "        # Used for upsampling instead of transposed convolutions\n",
    "        self.upsample = nn.Upsample(scale_factor = (2, 2))\n",
    "\n",
    "        # Convolutions (projection)\n",
    "        self.projection_1 = nn.Conv2d(in_channels = 32 * scaling * (8 + 4), out_channels = 32 * scaling * 4, kernel_size = 3, padding = 1)\n",
    "        self.projection_2 = nn.Conv2d(in_channels = 32 * scaling * (4 + 2), out_channels = 32 * scaling * 2, kernel_size = 3, padding = 1)\n",
    "        self.projection_3 = nn.Conv2d(in_channels = 32 * scaling * (2 + 1), out_channels = 32 * scaling    , kernel_size = 3, padding = 1)\n",
    "\n",
    "        # Time Residual Blocks (2)\n",
    "        self.upsample_11_residuals = TimeResidual_Block(input_channels = 32 * scaling * 4, frequencies = self.frequencies)\n",
    "        self.upsample_12_residuals = TimeResidual_Block(input_channels = 32 * scaling * 4, frequencies = self.frequencies)\n",
    "        self.upsample_21_residuals = TimeResidual_Block(input_channels = 32 * scaling * 2, frequencies = self.frequencies)\n",
    "        self.upsample_22_residuals = TimeResidual_Block(input_channels = 32 * scaling * 2, frequencies = self.frequencies)\n",
    "        self.upsample_31_residuals = TimeResidual_Block(input_channels = 32 * scaling    , frequencies = self.frequencies)\n",
    "        self.upsample_32_residuals = TimeResidual_Block(input_channels = 32 * scaling    , frequencies = self.frequencies)\n",
    "\n",
    "        # 4. Output (We use a linear to mix accross channels, a convolution mix spatially and introduce bias at the corners)\n",
    "        self.output_linear = nn.Linear(in_features = 32 * scaling, out_features = self.output_channels, bias = False)\n",
    "\n",
    "        # Normalization\n",
    "        self.normalization = LayerNormalization(dim = 1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "\n",
    "        # 1. Lifting\n",
    "        x = self.input_conv(x)\n",
    "\n",
    "        # 2. Downsampling\n",
    "        x = self.downsample_11_residuals(x, time)\n",
    "        x = self.downsample_12_residuals(x, time)\n",
    "        x1 = self.downsample_1_conv(x)\n",
    "        x1 = self.downsample_21_residuals(x1, time)\n",
    "        x1 = self.downsample_22_residuals(x1, time)\n",
    "\n",
    "        x2 = self.downsample_2_conv(x1)\n",
    "        x2 = self.downsample_31_residuals(x2, time)\n",
    "        x2 = self.downsample_32_residuals(x2, time)\n",
    "\n",
    "        x3 = self.downsample_3_conv(x2)\n",
    "        x3 = self.downsample_41_residuals(x3, time)\n",
    "        x3 = self.downsample_42_residuals(x3, time)\n",
    "        x3 = self.normalization(x3)\n",
    "\n",
    "        # 3. Upsampling\n",
    "        x3 = self.upsample(x3)\n",
    "\n",
    "        x2 = torch.cat([x3, x2], dim = 1)\n",
    "        x2 = self.projection_1(x2)\n",
    "        x2 = self.upsample_11_residuals(x2, time)\n",
    "        x2 = self.upsample_12_residuals(x2, time)\n",
    "        x2 = self.normalization(x2)\n",
    "        x2 = self.upsample(x2)\n",
    "\n",
    "        x1 = torch.cat([x2, x1], dim = 1)\n",
    "        x1 = self.projection_2(x1)\n",
    "        x1 = self.upsample_21_residuals(x1, time)\n",
    "        x1 = self.upsample_22_residuals(x1, time)\n",
    "        x1 = self.normalization(x1)\n",
    "        x1 = self.upsample(x1)\n",
    "\n",
    "        x = torch.cat([x1, x], dim = 1)\n",
    "        x = self.projection_3(x)\n",
    "        x = self.upsample_31_residuals(x, time)\n",
    "        x = self.upsample_32_residuals(x, time)\n",
    "\n",
    "        # 4. Output\n",
    "        x = self.output_linear(torch.permute(x, (0, 2, 3, 1)))\n",
    "\n",
    "        # 5. Adding separate channels for mean and log(var)\n",
    "        return torch.permute(x, (0, 3, 1, 2))\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "\n",
    "class Diffusion_UNET(nn.Module):\n",
    "    r\"\"\"A diffusion UNET for time series forecasting\"\"\"\n",
    "\n",
    "    def __init__(self, window_input: int,\n",
    "                      window_output:int,\n",
    "                    diffusion_steps: int,\n",
    "                diffusion_scheduler: float = 0.01511,\n",
    "                 diffusion_variance: float = 0.01,\n",
    "                            scaling: int = 1,\n",
    "                        frequencies: int = 32,\n",
    "                             device: str = 'cpu'):\n",
    "        super(Diffusion_UNET, self).__init__()\n",
    "\n",
    "        # Initialization\n",
    "        self.diffusion_steps     = diffusion_steps\n",
    "        self.diffusion_scheduler = diffusion_scheduler\n",
    "        self.diffusion_variance  = diffusion_variance\n",
    "        self.frequencies         = frequencies\n",
    "        self.device              = device\n",
    "\n",
    "        # ---- Pre-Calculation ----\n",
    "        #\n",
    "        # Diffusion steps and their encoding\n",
    "        steps_t            = torch.arange(1, self.diffusion_steps + 1, dtype = torch.float32)\n",
    "        self.encoded_steps = time_encoding(steps_t[:, None], self.frequencies)[:, 0]\n",
    "\n",
    "        # Constants\n",
    "        betas                      = torch.ones((self.diffusion_steps, 1), dtype = torch.float32) * diffusion_scheduler\n",
    "        alphas                     = torch.pow(1 - diffusion_scheduler, steps_t)[:, None]\n",
    "        self.sqrt_alphas           = torch.sqrt(alphas)\n",
    "        self.sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\n",
    "        self.latent_constant_zt    = 1 / (torch.sqrt(1 - betas))\n",
    "        self.latent_constant_noise = betas / (torch.sqrt(1 - alphas) * torch.sqrt(1 - betas))\n",
    "\n",
    "        # Pushing to the device\n",
    "        self.latent_constant_zt    = self.latent_constant_zt.to(self.device)\n",
    "        self.latent_constant_noise = self.latent_constant_noise.to(self.device)\n",
    "\n",
    "        # ---- Model ----\n",
    "        #\n",
    "        # Number of inputs (mesh (2), bathymetry (1), time (3), conditioning (4 * win) and the number of forecasted days (wout))\n",
    "        nb_inputs = 3 + 3 + 4 * window_input + window_output\n",
    "\n",
    "        # Model\n",
    "        self.model = TimeResidual_UNET(input_channels = nb_inputs, output_channels = window_output, frequencies = self.frequencies, scaling = scaling)\n",
    "\n",
    "    def parrelize(self, number_gpus: int):\n",
    "        self.model = torch.nn.parallel.DataParallel(self.model, device_ids= list(range(number_gpus)), dim= 0)\n",
    "\n",
    "    def count_parameters(self,):\n",
    "        r\"\"\"Determines the number of trainable parameters in the model\"\"\"\n",
    "        return self.model.count_parameters()\n",
    "\n",
    "    def predict(self, z, diffusion_steps):\n",
    "        return self.model(z, self.encoded_steps[diffusion_steps[:,0]].to(self.device))\n",
    "\n",
    "    def generate_latent(self, x, noise, diffusion_steps):\n",
    "        \"\"\"Used to generate the latent variable z_t given the input x\"\"\"\n",
    "\n",
    "        # Extracting constants\n",
    "        sqrt_alphas           = self.sqrt_alphas[diffusion_steps[:, 0]][:, :, None, None].expand(-1, -1, *x.shape[2:])\n",
    "        sqrt_one_minus_alphas = self.sqrt_one_minus_alphas[diffusion_steps[:, 0]][:, :, None, None].expand(-1, -1, *x.shape[2:])\n",
    "\n",
    "        # Computing the latent variable z_t\n",
    "        return sqrt_alphas * x + sqrt_one_minus_alphas * noise\n",
    "\n",
    "    def generate_samples(self, x, conditioning, number_trajectories: int = 3):\n",
    "        \"\"\"Given an input and its conditioning, generate multiple samples\"\"\"\n",
    "\n",
    "        # Libraries\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        # Making sure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Stores the generated samples\n",
    "            x_generated = list()\n",
    "\n",
    "            # Generating multiple trajectories\n",
    "            for n in tqdm(range(number_trajectories)):\n",
    "\n",
    "                # Sampling the initial noise\n",
    "                zt = torch.normal(0, 1, x.shape, device = self.device)\n",
    "\n",
    "                # Reverse process\n",
    "                for t in range(self.diffusion_steps - 1, 0, -1):\n",
    "\n",
    "                    # Genereting encoded timesteps\n",
    "                    diffusion_steps = torch.ones(x.shape[0], 1, dtype=torch.int64) * t\n",
    "\n",
    "                    # Removing the noise\n",
    "                    zt_hat = self.latent_constant_zt[t] * zt - self.latent_constant_noise[t] * self.predict(torch.cat([conditioning, zt], dim = 1), diffusion_steps)\n",
    "\n",
    "                    # Generating noise\n",
    "                    noise = torch.normal(0, 1, zt_hat.shape).to(self.device)\n",
    "\n",
    "                    # Adding a bit of noise for stochasticity but not on the last step\n",
    "                    zt = zt_hat + self.diffusion_variance * noise if t > 1 else zt_hat\n",
    "\n",
    "                # Adding the final sample\n",
    "                x_generated.append(zt)\n",
    "\n",
    "            # Returning the generated samples\n",
    "            return torch.stack(x_generated, dim = 2)\n",
    "\n",
    "\n",
    "# ---------------\n",
    "#\n",
    "# Initialization\n",
    "neural_net = Diffusion_UNET(window_input, window_output, diff_steps, diff_scheduler, diff_variance, scaling, frequencies, device).to(device)\n",
    "num_gpus   = torch.cuda.device_count()\n",
    "neural_net.parrelize(num_gpus)\n",
    "\n",
    "\"\"\"\n",
    "# Computing Metrics\n",
    "for c, _, x in dataloader_valid:\n",
    "\n",
    "    # Pushing to device\n",
    "    x, c = x.to(device), c.to(device)\n",
    "\n",
    "    # Generating conditionnal samples\n",
    "    x = neural_net.generate_samples(x = x, conditioning = c, number_trajectories = 10)\n",
    "\n",
    "    # Computing Metrics\n",
    "\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------—--------------------\n",
    "#     Neural Network & Training\n",
    "# -------------—--------------------\n",
    "#\n",
    "# Initialization\n",
    "neural_net = Diffusion_UNET(window_input, window_output, diff_steps, diff_scheduler, diff_variance, scaling, frequencies, device).to(device)\n",
    "\n",
    "# Training Parameters\n",
    "optimizer  = optim.Adam(neural_net.parameters(), lr = learning_rate)\n",
    "scheduler  = LinearLR(optimizer, start_factor = 0.95, total_iters = nb_epochs)\n",
    "\n",
    "# Information about the model\n",
    "num_gpus  = torch.cuda.device_count()\n",
    "nn_params = neural_net.count_parameters()\n",
    "\n",
    "# Deploying the model on multiple GPUs\n",
    "neural_net.parrelize(num_gpus)\n",
    "\n",
    "# Displaying information over the terminal\n",
    "print(\"Total number of parameters: \", nn_params/1e6, \"M\")\n",
    "print(\"Available GPUs: \", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB (1) - Initialization of the run\n",
    "wandb.init(project = project, mode = mode, config = kwargs)\n",
    "\n",
    "# WandB (2) - Logging info\n",
    "wandb.config.update({\"Number of Parameters\": nn_params, \"Number of GPUs\": num_gpus})\n",
    "\n",
    "# ------- Training Loop -------\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    # Stores the mean loss\n",
    "    mean_loss = list()\n",
    "\n",
    "    for conditioning, _, x in dataloader_train:\n",
    "\n",
    "        # ------ Preprocessing -----\n",
    "        #\n",
    "        # Sampling uniformly diffusion steps\n",
    "        diffusion_steps = torch.randint(0, diff_steps, (x.shape[0], 1))\n",
    "\n",
    "        # Sampling noise\n",
    "        noise = torch.normal(0, 1, x.shape)\n",
    "\n",
    "        # Generating latent representations of the data\n",
    "        z_t = neural_net.generate_latent(x, noise, diffusion_steps)\n",
    "\n",
    "        # Pushing to device\n",
    "        z_t, conditioning, noise, diffusion_steps =  z_t.to(device), conditioning.to(device),  noise.to(device), diffusion_steps\n",
    "\n",
    "        # Adding the conditioning\n",
    "        z_t = torch.cat([conditioning, z_t], dim = 1)\n",
    "\n",
    "        # ----- Training -----\n",
    "        #\n",
    "        # Predicting the noise\n",
    "        noise_pred = neural_net.predict(z_t, diffusion_steps)\n",
    "\n",
    "        # Computing the loss (MSE between noise levels)\n",
    "        loss = torch.pow(noise_pred[:, :, black_sea_mask_cs[0] == 1] - noise[:, :, black_sea_mask_cs[0] == 1], 2).nanmean()\n",
    "\n",
    "        # Appending the loss\n",
    "        mean_loss.append(loss.item())\n",
    "\n",
    "        # WandB (4) - Logging the loss\n",
    "        wandb.log({\"Training/Loss (Instantaneous)\": loss.item()})\n",
    "\n",
    "        # Optimizing\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Displaying the loss\n",
    "        print(\"Loss: \", loss.item())\n",
    "\n",
    "        break\n",
    "\n",
    "    # WandB (3) - Logging the loss and the epoch\n",
    "    wandb.log({\"Training/Loss (Averaged Over Batch)\": np.mean(mean_loss), \"Epoch (Left)\": nb_epochs - epoch})\n",
    "\n",
    "    # Updating the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Computing Metrics\n",
    "    for c, _, x in dataloader_valid:\n",
    "\n",
    "        # Pushing to device\n",
    "        x, c = x.to(device), c.to(device)\n",
    "\n",
    "        # Generating conditionnal samples\n",
    "        forecast = neural_net.generate_samples(x = x, conditioning = c, number_trajectories = 24)\n",
    "\n",
    "        # Computing Metrics\n",
    "        metrics(x.cpu(), forecast.cpu(), black_sea_mask_cs, hypoxia_treshold_standardized)\n",
    "\n",
    "        # Only on the first batch (= one year of data)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97283a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ground_truth: torch.Tensor, forecasts: torch.Tensor, mask: torch.Tensor, treshold: float):\n",
    "    \"\"\"Used to compute different visualizations\"\"\"\n",
    "\n",
    "    def plot_forecasts(ground_truth: torch.Tensor, forecasts: torch.Tensor, mask: torch.Tensor, day: int = 0):\n",
    "        \"\"\"Plotting the forecasts against the ground truth\"\"\"\n",
    "\n",
    "        # Plotting the forecasts\n",
    "        fig, ax = plt.subplots(12, 4, figsize = (30, 50))\n",
    "\n",
    "        # List of months\n",
    "        months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "        # Looping over each first day of the month\n",
    "        for i in range(12):\n",
    "\n",
    "            # Extracting the region of interest\n",
    "            gt = ground_truth[i, day, 25:125, 70:270]\n",
    "\n",
    "            # Masking the ground truth\n",
    "            gt[mask[0, 25:125, 70:270] == 0] = np.nan\n",
    "\n",
    "            # Extracting the minimum and maximum values\n",
    "            vmin, vmax = np.nanmin(gt), np.nanmax(gt)\n",
    "\n",
    "            # Plotting the ground truth\n",
    "            ax[i, 0].imshow(gt, label = \"Ground Truth\")\n",
    "\n",
    "            # Removing the tickz\n",
    "            ax[i, 0].set_xticks([])\n",
    "            ax[i, 0].set_yticks([])\n",
    "\n",
    "            # Adding the title\n",
    "            ax[i, 0].set_ylabel(months[i])\n",
    "\n",
    "            # Plotting the forecasts\n",
    "            for j in range(3):\n",
    "\n",
    "                # Extracting the forecast\n",
    "                fc = forecasts[i, day, j, 25:125, 70:270]\n",
    "\n",
    "                # Masking the forecast\n",
    "                fc[mask[0, 25:125, 70:270] == 0] = np.nan\n",
    "\n",
    "                # Plotting the forecast\n",
    "                ax[i, j + 1].imshow(fc, label = f\"Forecast {j + 1}\", vmin = vmin, vmax = vmax)\n",
    "                ax[i, j + 1].set_xticks([])\n",
    "                ax[i, j + 1].set_yticks([])\n",
    "\n",
    "        # Tight layout\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def plot_hypoxia(ground_truth: torch.Tensor, forecasts: torch.Tensor, mask: torch.Tensor, day: int = 0):\n",
    "        \"\"\"Plotting the forecasts against the ground truth\"\"\"\n",
    "\n",
    "        # Plotting the forecasts\n",
    "        fig, ax = plt.subplots(12, 4, figsize = (30, 50))\n",
    "\n",
    "        # List of months\n",
    "        months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "        # Looping over each first day of the month\n",
    "        for i in range(12):\n",
    "\n",
    "            # Extracting the region of interest\n",
    "            gt = ground_truth[i, day, 25:125, 70:270]\n",
    "\n",
    "            # Masking the ground truth\n",
    "            gt[mask[0, 25:125, 70:270] == 0] = -1\n",
    "\n",
    "            # Extracting the minimum and maximum values\n",
    "            vmin, vmax = np.nanmin(gt), np.nanmax(gt)\n",
    "\n",
    "            # Plotting the ground truth\n",
    "            ax[i, 0].imshow(gt, label = \"Ground Truth\")\n",
    "\n",
    "            # Removing the tickz\n",
    "            ax[i, 0].set_xticks([])\n",
    "            ax[i, 0].set_yticks([])\n",
    "\n",
    "            # Adding the title\n",
    "            ax[i, 0].set_ylabel(months[i])\n",
    "\n",
    "            # Plotting the forecasts\n",
    "            for j in range(3):\n",
    "\n",
    "                # Extracting the forecast\n",
    "                fc = forecasts[i, day, j, 25:125, 70:270]\n",
    "\n",
    "                # Masking the forecast\n",
    "                fc[mask[0, 25:125, 70:270] == 0] = -1\n",
    "\n",
    "                # Plotting the forecast\n",
    "                ax[i, j + 1].imshow(fc, label = f\"Forecast {j + 1}\", vmin = vmin, vmax = vmax)\n",
    "                ax[i, j + 1].set_xticks([])\n",
    "                ax[i, j + 1].set_yticks([])\n",
    "\n",
    "        # Tight layout\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def plot_probability_maps(ground_truth: torch.Tensor, forecasts: torch.Tensor, mask: torch.Tensor, day: int = 0):\n",
    "        \"\"\"Plotting the probability maps\"\"\"\n",
    "\n",
    "        # Plotting the forecasts\n",
    "        fig, ax = plt.subplots(12, 2, figsize = (10, 30))\n",
    "\n",
    "        # Computing probability map\n",
    "        prob_map = torch.mean(forecasts, dim = 2)\n",
    "\n",
    "        # List of months\n",
    "        months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "        # Looping over each first day of the month\n",
    "        for i in range(12):\n",
    "\n",
    "            # Extracting the region of interest\n",
    "            gt = ground_truth[i, day, 25:125, 70:270]\n",
    "            pm = prob_map[i, day, 25:125, 70:270]\n",
    "\n",
    "            # Masking the ground truth\n",
    "            gt[mask[0, 25:125, 70:270] == 0] = np.nan\n",
    "            pm[mask[0, 25:125, 70:270] == 0] = np.nan\n",
    "\n",
    "            # Extracting the minimum and maximum values\n",
    "            vmin, vmax = np.nanmin(gt), np.nanmax(gt)\n",
    "\n",
    "            # Plotting the ground truth\n",
    "            ax[i, 0].imshow(gt, label = \"Ground Truth\")\n",
    "\n",
    "            # Adding the colorbar\n",
    "            fig.colorbar(ax[i, 1].imshow(pm, vmin = 0, vmax = 1, cmap=\"inferno\"), ax = ax[i, 1])\n",
    "\n",
    "            # Removing the tickz\n",
    "            ax[i, 0].set_xticks([])\n",
    "            ax[i, 0].set_yticks([])\n",
    "            ax[i, 1].set_xticks([])\n",
    "            ax[i, 1].set_yticks([])\n",
    "\n",
    "            # Adding the title\n",
    "            ax[i, 0].set_ylabel(months[i])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def compute_precision_recall(ground_truth: torch.Tensor, forecasts: torch.Tensor, mask: torch.Tensor):\n",
    "        \"\"\"Computes the recall and precision of the forecasts\"\"\"\n",
    "\n",
    "        # Store results\n",
    "        results = {\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'accuracy': []\n",
    "        }\n",
    "\n",
    "        # Define thresholds\n",
    "        thresholds = [i / 10 for i in range(0, 11)]\n",
    "\n",
    "        # Computing probability map\n",
    "        prob_map = torch.mean(forecasts, dim = 2)\n",
    "\n",
    "        # Extracting only relevant information\n",
    "        ground_truth = ground_truth[:, :, mask[0] == 1]\n",
    "        prob_map     = prob_map[:, :, mask[0] == 1]\n",
    "\n",
    "        # Computing metrics\n",
    "        for threshold in thresholds:\n",
    "\n",
    "            # Binarize predictions\n",
    "            binary_prediction = (prob_map >= threshold).float()\n",
    "\n",
    "            # Calculate TP, FP, TN, FN\n",
    "            TP = (binary_prediction * ground_truth).sum(dim=(0, 2))\n",
    "            FP = (binary_prediction * (1 - ground_truth)).sum(dim=(0, 2))\n",
    "            TN = ((1 - binary_prediction) * (1 - ground_truth)).sum(dim=(0, 2))\n",
    "            FN = ((1 - binary_prediction) * ground_truth).sum(dim=(0, 2))\n",
    "\n",
    "            # Compute precision, recall, and accuracy\n",
    "            precision =       TP / (TP + FP + 1e-8)\n",
    "            recall =          TP / (TP + FN + 1e-8)\n",
    "            accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-8)\n",
    "\n",
    "            # Adding results to the dictionary\n",
    "            results['precision'].append(precision)\n",
    "            results['recall'].append(recall)\n",
    "            results['accuracy'].append(accuracy)\n",
    "\n",
    "        # Convert lists to tensors for better handling\n",
    "        results['precision'] = torch.stack(results['precision'], dim=0)\n",
    "        results['recall'] = torch.stack(results['recall'], dim=0)\n",
    "        results['accuracy'] = torch.stack(results['accuracy'], dim=0)\n",
    "\n",
    "        # Computing Recall vs Precision curve\n",
    "        rec_pre_0 = plt.figure(figsize=(7, 7))\n",
    "        plt.plot(results['recall'][:, 0], results['precision'][:, 0], marker='*')\n",
    "        plt.xlabel('Recall [-]')\n",
    "        plt.ylabel('Precision [-]')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid()\n",
    "\n",
    "        rec_pre_1 = plt.figure(figsize=(7, 7))\n",
    "        plt.plot(results['recall'][:, -1], results['precision'][:,-1], marker='o')\n",
    "        plt.xlabel('Recall [-]')\n",
    "        plt.ylabel('Precision [-]')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid()\n",
    "\n",
    "        # Sending results to WandB\n",
    "        wandb.log({\"Precision-Recall Curve (First Day)\": wandb.Image(rec_pre_0),\n",
    "                   \"Precision-Recall Curve (Last Day)\":  wandb.Image(rec_pre_1)})\n",
    "\n",
    "        # Sending singular values and averages\n",
    "        for i, t in enumerate(thresholds):\n",
    "            wandb.log({f\"Metrics/Precision (First Day, T = {t}))\" : results['precision'][i, 0],\n",
    "                       f\"Metrics/Precision (Last Day, T = {t}))\"  : results['precision'][i, -1],\n",
    "                       f\"Metrics/Recall (First Day, T = {t}))\"    : results['recall'][i, 0],\n",
    "                       f\"Metrics/Recall (Last Day, T = {t}))\"     : results['recall'][i, -1],\n",
    "                       f\"Metrics/Accuracy (First Day, T = {t}))\"  : results['accuracy'][i, 0],\n",
    "                       f\"Metrics/Accuracy (Last Day, T = {t}))\"   : results['accuracy'][i, -1]})\n",
    "\n",
    "    # Plotting the forecasts\n",
    "    wandb.log({\"Forecast Visualization (First Day)\": wandb.Image(plot_forecasts(ground_truth, forecasts, mask, day = 0))})\n",
    "    wandb.log({\"Forecast Visualization (Last Day)\":  wandb.Image(plot_forecasts(ground_truth, forecasts, mask, day = -1))})\n",
    "\n",
    "    # Extracting hypoxia regions\n",
    "    ground_truth = (ground_truth < treshold) * 1.0\n",
    "    forecasts    = (forecasts < treshold) * 1.0\n",
    "\n",
    "    # Plotting the hypoxia regions\n",
    "    wandb.log({\"Hypoxia Visualization (First Day)\": wandb.Image(plot_hypoxia(ground_truth, forecasts, mask, day = 0))})\n",
    "    wandb.log({\"Hypoxia Visualization (Last Day)\": wandb.Image(plot_hypoxia(ground_truth, forecasts, mask, day = -1))})\n",
    "\n",
    "    # Plotting the probability maps\n",
    "    wandb.log({\"Probability Visualization (First Day)\": wandb.Image(plot_probability_maps(ground_truth, forecasts, mask, day = 0))})\n",
    "    wandb.log({\"Probability Visualization (Last Day)\": wandb.Image(plot_probability_maps(ground_truth, forecasts, mask, day = -1))})\n",
    "\n",
    "    # Computing global metrics\n",
    "    compute_precision_recall(ground_truth, forecasts, mask)\n",
    "\n",
    "\n",
    "metrics(x.cpu(), forecast.cpu() , black_sea_mask_cs, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
